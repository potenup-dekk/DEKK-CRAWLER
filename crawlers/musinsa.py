import time
import json
import requests
import random

from curl_cffi import requests as curl_requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from .base import BaseCrawler
from core.logger import logger

class MusinsaCrawler(BaseCrawler):
    platform_name = "MUSINSA"
    
    def fetch_new_snaps(self, last_snap_id, max_scrolls=5):
        """Playwrightë¡œ ìŠ¤í¬ë¡¤í•˜ë©° last_snap_id ì´ì „ê¹Œì§€ì˜ ì‹ ê·œ ìŠ¤ëƒ… IDë§Œ ì¶”ì¶œ (Delta Crawling)"""
        logger.info(f"[{self.platform_name}] ì‹ ê·œ ìŠ¤ëƒ… íƒìƒ‰ ì‹œì‘ (ë§ˆì§€ë§‰ ID: {last_snap_id})")

        new_ids = []
        seen = set()

        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage'
                ]
            )

            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={'width': 1920, 'height': 1080},
            )

            page = context.new_page()
            page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            url = "https://www.musinsa.com/snap/main/recommend?gf=A&sort=NEWEST"
            page.goto(url)

            try:
                page.wait_for_selector("a[href*='/snap/']", timeout=10000)
            except Exception as e:
                logger.error(f"í˜ì´ì§€ ë¡œë”© ë˜ëŠ” ë´‡ ì°¨ë‹¨ ë°œìƒ: {e}")
                browser.close()
                return []

            found_last = False
            for _ in range(max_scrolls):
                links = page.locator("a[href*='/snap/']").all()
                for link in links:
                    href = link.get_attribute("href")
                    if not href:
                        continue

                    snap_id = href.split('/snap/')[-1].split('?')[0]
                    if not snap_id.isdigit():
                        continue

                    if snap_id == last_snap_id:
                        found_last = True
                        break

                    if snap_id not in seen:
                        seen.add(snap_id)
                        new_ids.append(snap_id)

                if found_last:
                    break

                page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(random.uniform(1.5, 3.0))

            browser.close()

        logger.info(f"[{self.platform_name}] ìŠ¤ëƒ… íƒìƒ‰ ì™„ë£Œ: {len(new_ids)}ê°œ ë°œê²¬")
        return new_ids[::-1]
    
    def _fetch_goods_batch(self, goods_nos):
        if not goods_nos: return []
        formatted_ids = ",".join([f"MUSINSA:{gn}" for gn in goods_nos])
        url = f"https://content.musinsa.com/api2/content/snap/v1/goods?goodsIds={formatted_ids}"
        
        try:
            res = curl_requests.get(url, impersonate="chrome110", timeout=10).json()
            return res.get('data', {}).get('list', [])
        except Exception:
            return []

    def process_and_upload(self, snap_id):
        url = f"https://www.musinsa.com/snap/{snap_id}"
        
        # ğŸ’¡ ê° ìŠ¤ë ˆë“œ íœ´ì‹ì€ ë°©í™”ë²½ íšŒí”¼ë¥¼ ìœ„í•´ ê¼­ ìœ ì§€í•´ ì£¼ì„¸ìš”
        time.sleep(random.uniform(1.5, 3.5)) 

        try:
            response = curl_requests.get(
                url, 
                impersonate="chrome110",
                timeout=15 
            )
        except Exception as e:
            logger.error(f"ìŠ¤ëƒ… {snap_id} ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì‹¤íŒ¨: {e}")
            raise Exception("ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬")

        soup = BeautifulSoup(response.text, 'html.parser')
        script_tag = soup.find('script', id='__NEXT_DATA__')
        
        if not script_tag:
            logger.error(f"ìŠ¤ëƒ… {snap_id} NEXT_DATA ì—†ìŒ. ìƒíƒœì½”ë“œ: {response.status_code}")
            raise Exception("NEXT_DATA íŒŒì‹± ì‹¤íŒ¨")

        json_data = json.loads(script_tag.string)
        queries = json_data.get('props', {}).get('pageProps', {}).get('dehydratedState', {}).get('queries', [])
        
        raw_snap_data = {}
        for q in queries:
            if 'contentAPI.getApi2SnapSnapsByIdV1' in q.get('queryKey', []):
                raw_snap_data = q.get('state', {}).get('data', {}).get('data', {})
                break

        if not raw_snap_data: raise Exception("ìŠ¤ëƒ… ìƒì„¸ ì›ë³¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        goods_list_raw = raw_snap_data.get('goods', [])
        goods_nos = [str(g.get('goodsNo')) for g in goods_list_raw if g.get('goodsNo')]
        raw_snap_data['goods_detail_list'] = self._fetch_goods_batch(goods_nos)
        return raw_snap_data
